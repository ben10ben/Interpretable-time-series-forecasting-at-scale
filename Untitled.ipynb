{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a406ba2-6171-4248-b20c-811dce35add5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  \n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer, TorchNormalizer, NaNLabelEncoder\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import electricity_dataloader\n",
    "from config import *\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21a54180-1b53-4a00-b3cf-eca56a295178",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>power_usage</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>categorical_id</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>categorical_day_of_week</th>\n",
       "      <th>categorical_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17544</td>\n",
       "      <td>2.538071</td>\n",
       "      <td>26304</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26304.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17545</td>\n",
       "      <td>2.855330</td>\n",
       "      <td>26305</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17546</td>\n",
       "      <td>2.855330</td>\n",
       "      <td>26306</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26306.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17547</td>\n",
       "      <td>2.855330</td>\n",
       "      <td>26307</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 03:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26307.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17548</td>\n",
       "      <td>2.538071</td>\n",
       "      <td>26308</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 04:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26308.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  power_usage  time_idx  days_from_start categorical_id  \\\n",
       "0       17544     2.538071     26304             1096         MT_001   \n",
       "1       17545     2.855330     26305             1096         MT_001   \n",
       "2       17546     2.855330     26306             1096         MT_001   \n",
       "3       17547     2.855330     26307             1096         MT_001   \n",
       "4       17548     2.538071     26308             1096         MT_001   \n",
       "\n",
       "                  date      id  hour  day  day_of_week  month  \\\n",
       "0  2014-01-01 00:00:00  MT_001     0    1            2      1   \n",
       "1  2014-01-01 01:00:00  MT_001     1    1            2      1   \n",
       "2  2014-01-01 02:00:00  MT_001     2    1            2      1   \n",
       "3  2014-01-01 03:00:00  MT_001     3    1            2      1   \n",
       "4  2014-01-01 04:00:00  MT_001     4    1            2      1   \n",
       "\n",
       "   hours_from_start  categorical_day_of_week  categorical_hour  \n",
       "0           26304.0                        2                 0  \n",
       "1           26305.0                        2                 1  \n",
       "2           26306.0                        2                 2  \n",
       "3           26307.0                        2                 3  \n",
       "4           26308.0                        2                 4  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_file = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014.csv\"\n",
    "electricity = pd.read_csv(csv_file)\n",
    "  \n",
    "config_name_string = \"electricity\"\n",
    "model_dir = CONFIG_DICT[\"models\"][config_name_string]\n",
    "  \n",
    "    \n",
    "electricity['time_idx'] = electricity['time_idx'].astype('int')\n",
    "electricity[\"categorical_id\"] = electricity['categorical_id'].astype('string').astype(\"category\")\n",
    "electricity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b20c9ee-e59b-4fd4-a72b-807d9f08ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prediction_length = 24\n",
    "max_encoder_length = 168\n",
    "    \n",
    "valid_boundary=1315\n",
    "test_boundary=1339\n",
    "    \n",
    "index = electricity['days_from_start']\n",
    "train = electricity.loc[index < valid_boundary]\n",
    "valid = electricity.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "test = electricity.loc[index >= test_boundary - 7]\n",
    "    \n",
    "training_cutoff = electricity[\"time_idx\"].max() - max_prediction_length\n",
    "   \n",
    "training = TimeSeriesDataSet(\n",
    "    train[lambda x: x.time_idx <= training_cutoff],\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"power_usage\",\n",
    "    group_ids=[\"id\"],\n",
    "    min_encoder_length=max_encoder_length,\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=max_prediction_length,\n",
    "    max_prediction_length=max_prediction_length,\n",
    "    static_categoricals=[\"categorical_id\"],\n",
    "    static_reals=[],\n",
    "    time_varying_known_categoricals=[],\n",
    "    time_varying_known_reals=[\"time_idx\", \"hour\", \"day_of_week\"],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\"power_usage\"],\n",
    "    #target_normalizer=TorchNormalizer(method=\"standard\").fit(train[\"power_usage\"]),\n",
    "    target_normalizer=GroupNormalizer(groups=[\"id\"]),\n",
    "    scalers = {\"time_idx\" : StandardScaler(), \"hour\" : StandardScaler(), \"day_of_week\": StandardScaler()},\n",
    "    categorical_encoders= {'__group_id__id': NaNLabelEncoder(add_nan=False, warn=True), 'categorical_id': NaNLabelEncoder(add_nan=False, warn=True)},\n",
    "    add_relative_time_idx=False,\n",
    "    add_target_scales=True,\n",
    "    add_encoder_length=False,\n",
    "#    lags= {\"power_usage\": [(24)]}\n",
    ")\n",
    "\n",
    "# get parameters from train dataset to create val/test\n",
    "model_parameters = training.get_parameters()\n",
    "\n",
    "testing = TimeSeriesDataSet.from_parameters(parameters=model_parameters, data=test, predict=True, stop_randomization=True)\n",
    "validating = TimeSeriesDataSet.from_parameters(parameters=model_parameters, data=valid, predict=True,stop_randomization=True)\n",
    "    \n",
    "\n",
    "# create dataloaders for model\n",
    "batch_size = 64\n",
    "    \n",
    "train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=10, pin_memory=True)\n",
    "test_dataloader = testing.to_dataloader(train=False, batch_size=batch_size, num_workers=10, pin_memory=True)\n",
    "val_dataloader = validating.to_dataloader(train=False, batch_size=batch_size, num_workers=10, pin_memory=True)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c58907-e46b-4aed-b302-da64a5f59551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:268: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\callbacks\\model_checkpoint.py:616: UserWarning: Checkpoint directory C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\models\\electricity\\checkpoint_callback_logs exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | logging_metrics                    | ModuleList                      | 0     \n",
      "2  | input_embeddings                   | MultiEmbedding                  | 16.2 K\n",
      "3  | prescalers                         | ModuleDict                      | 384   \n",
      "4  | static_variable_selection          | VariableSelectionNetwork        | 23.3 K\n",
      "5  | encoder_variable_selection         | VariableSelectionNetwork        | 46.2 K\n",
      "6  | decoder_variable_selection         | VariableSelectionNetwork        | 34.5 K\n",
      "7  | static_context_variable_selection  | GatedResidualNetwork            | 66.3 K\n",
      "8  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 66.3 K\n",
      "9  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 66.3 K\n",
      "10 | static_context_enrichment          | GatedResidualNetwork            | 66.3 K\n",
      "11 | lstm_encoder                       | LSTM                            | 132 K \n",
      "12 | lstm_decoder                       | LSTM                            | 132 K \n",
      "13 | post_lstm_gate_encoder             | GatedLinearUnit                 | 33.0 K\n",
      "14 | post_lstm_add_norm_encoder         | AddNorm                         | 256   \n",
      "15 | static_enrichment                  | GatedResidualNetwork            | 82.7 K\n",
      "16 | multihead_attn                     | InterpretableMultiHeadAttention | 49.5 K\n",
      "17 | post_attn_gate_norm                | GateAddNorm                     | 33.3 K\n",
      "18 | pos_wise_ff                        | GatedResidualNetwork            | 66.3 K\n",
      "19 | pre_output_gate_norm               | GateAddNorm                     | 33.3 K\n",
      "20 | output_layer                       | Linear                          | 387   \n",
      "----------------------------------------------------------------------------------------\n",
      "948 K     Trainable params\n",
      "0         Non-trainable params\n",
      "948 K     Total params\n",
      "3.792     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Definining TFT...\n",
      "Number of parameters in network: 948.1k\n",
      "Training model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49e77a84633452c9feef440b74559ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    accelerator = \"gpu\"\n",
    "    devices = find_usable_cuda_devices(1)\n",
    "else:\n",
    "    accelerator = \"cpu\"\n",
    "    devices = None\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(save_top_k=3, monitor=\"val_loss\", mode=\"min\",\n",
    "          dirpath=CONFIG_DICT[\"models\"][\"electricity\"] / \"checkpoint_callback_logs\",\n",
    "          filename=\"sample-mnist-{epoch:02d}-{val_loss:.2f}\")\n",
    "  \n",
    "writer = SummaryWriter(log_dir = CONFIG_DICT[\"models\"][\"electricity\"] / \"writer_logs\" )\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=3, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch') \n",
    "logger = TensorBoardLogger(CONFIG_DICT[\"models\"][\"electricity\"]) \n",
    "  \n",
    "  # best parameters estimated by hypertuning and manually rounded\n",
    "hyper_dict = {\n",
    "                'gradient_clip_val': 0.052, \n",
    "                'hidden_size': 128, \n",
    "                'dropout': 0.15, \n",
    "                'hidden_continuous_size': 32, \n",
    "                'attention_head_size': 2, \n",
    "                'learning_rate': 0.007,\n",
    "             }\n",
    "  \n",
    "# uncomment to read hyperparamters from hyper-tuning script\n",
    "#hyper_dict = pd.read_pickle(CONFIG_DICT[\"models\"][\"electricity\"] / \"tuning_logs\" / \"hypertuning_electricity.pkl\")\n",
    "  \n",
    "trainer = pl.Trainer(\n",
    "      default_root_dir=model_dir,\n",
    "      max_epochs=10,\n",
    "      devices=devices,\n",
    "      accelerator=accelerator,\n",
    "      enable_model_summary=True,\n",
    "      gradient_clip_val=hyper_dict[\"gradient_clip_val\"],\n",
    "      fast_dev_run=False,  \n",
    "      callbacks=[lr_logger, early_stop_callback, checkpoint_callback],\n",
    "      log_every_n_steps=1,\n",
    "      logger=logger,\n",
    "      profiler=\"simple\",\n",
    "    )\n",
    "\n",
    "print(\"Definining TFT...\")\n",
    "  \n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "      training,\n",
    "      learning_rate=hyper_dict[\"learning_rate\"],\n",
    "      hidden_size=hyper_dict[\"hidden_size\"],\n",
    "      attention_head_size=hyper_dict[\"attention_head_size\"],\n",
    "      dropout=hyper_dict[\"dropout\"],\n",
    "      hidden_continuous_size=hyper_dict[\"hidden_continuous_size\"],\n",
    "      output_size= 3,\n",
    "      loss=QuantileLoss([0.1, 0.5, 0.9]),\n",
    "      log_interval=1,\n",
    "      reduce_on_plateau_patience=4,\n",
    "      optimizer=\"adam\"\n",
    "    )\n",
    "\n",
    "trainer.optimizer = Adam(tft.parameters(), lr=hyper_dict[\"learning_rate\"])\n",
    "scheduler = ReduceLROnPlateau(trainer.optimizer, factor=0.2)  \n",
    "  \n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")\n",
    "print(\"Training model...\")\n",
    "  \n",
    "  # fit network\n",
    "trainer.fit(\n",
    "      tft,\n",
    "      train_dataloaders=train_dataloader,\n",
    "      val_dataloaders=val_dataloader,\n",
    "      #ckpt=\"~/RT1_TFT/models/electricity/lightning_logs/version_28/checkpoints/\"\n",
    ")\n",
    "\n",
    "  # safe model for later use\n",
    "torch.save(tft.state_dict(), CONFIG_DICT[\"models\"][\"electricity\"] / \"tft_model\")\n",
    "  \n",
    "print(\"trainging done. Evaluating...\")\n",
    "\n",
    "output = trainer.test(model=tft, dataloaders=test_dataloader , ckpt_path=\"best\")\n",
    "\n",
    "with open(CONFIG_DICT[\"models\"][\"electricity\"] / \"tuning_logs\" / \"tft_electricity_test_output_internal_scaling.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(output, fout)\n",
    "\n",
    "print(\"Done.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
