{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b496d73-f197-4f04-b2e0-2bf06518179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, DeviceStatsMonitor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import retail_dataloader, retail_formatter\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ac9adb-4358-4610-ae29-24a67cb5b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\dataloading_helpers\\retail_dataloader.py:392: DtypeWarning: Columns (5,6,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  retail_data = pd.read_csv(csv_file, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "retail = retail_dataloader.create_retail_timeseries_tft()\n",
    "timeseries_dict =  retail\n",
    "config_name_string = \"retail\"\n",
    "parameters = []\n",
    "model_dir = CONFIG_DICT[\"models\"][config_name_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c362b848-fa14-4a0b-87ca-4cccf9b150fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c1af6a-a1f7-4651-b212-03d871453b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train-valid-test splits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\dataloading_helpers\\retail_formatter.py:84: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  valid_boundary = pd.datetime(2015, 12, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting scalers with training data...\n",
      "Setting scalers with training data...\n"
     ]
    }
   ],
   "source": [
    "standardizer = retail_formatter.FavoritaFormatter()\n",
    "train, test, validation = standardizer.split_data(df=retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6281bdb0-8e99-4d48-ab6f-850c49258fa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>store_nbr</th>\n",
       "      <th>item_nbr</th>\n",
       "      <th>unit_sales</th>\n",
       "      <th>onpromotion</th>\n",
       "      <th>traj_id</th>\n",
       "      <th>unique_id</th>\n",
       "      <th>open</th>\n",
       "      <th>date</th>\n",
       "      <th>log_sales</th>\n",
       "      <th>oil</th>\n",
       "      <th>...</th>\n",
       "      <th>family</th>\n",
       "      <th>class</th>\n",
       "      <th>perishable</th>\n",
       "      <th>transactions</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>day_of_month</th>\n",
       "      <th>month</th>\n",
       "      <th>national_hol</th>\n",
       "      <th>regional_hol</th>\n",
       "      <th>local_hol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9280241</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_1003679</td>\n",
       "      <td>1_1003679_2015-08-21 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-21</td>\n",
       "      <td>-0.701294</td>\n",
       "      <td>-1.483286</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.091130</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280242</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_1003679</td>\n",
       "      <td>1_1003679_2015-08-22 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-22</td>\n",
       "      <td>-1.415077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.497176</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280246</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_1003679</td>\n",
       "      <td>1_1003679_2015-08-26 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>-1.798803</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.055483</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280247</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_1003679</td>\n",
       "      <td>1_1003679_2015-08-27 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-27</td>\n",
       "      <td>-1.415077</td>\n",
       "      <td>-1.156443</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.221101</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "      <td>8</td>\n",
       "      <td>54</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9280248</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1_1003679</td>\n",
       "      <td>1_1003679_2015-08-28 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-08-28</td>\n",
       "      <td>0.012490</td>\n",
       "      <td>-0.700158</td>\n",
       "      <td>...</td>\n",
       "      <td>10</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.194302</td>\n",
       "      <td>4</td>\n",
       "      <td>28</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263437</th>\n",
       "      <td>1</td>\n",
       "      <td>2244</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2_999547</td>\n",
       "      <td>2_999547_2016-01-30 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-01-30</td>\n",
       "      <td>-0.701294</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.938227</td>\n",
       "      <td>5</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263438</th>\n",
       "      <td>1</td>\n",
       "      <td>2244</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2_999547</td>\n",
       "      <td>2_999547_2016-01-31 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-01-31</td>\n",
       "      <td>-1.415077</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.813335</td>\n",
       "      <td>6</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263439</th>\n",
       "      <td>1</td>\n",
       "      <td>2244</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2_999547</td>\n",
       "      <td>2_999547_2016-02-01 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-02-01</td>\n",
       "      <td>0.588766</td>\n",
       "      <td>-2.912010</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433227</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263440</th>\n",
       "      <td>1</td>\n",
       "      <td>2244</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2_999547</td>\n",
       "      <td>2_999547_2016-02-02 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-02-02</td>\n",
       "      <td>1.226235</td>\n",
       "      <td>-3.190312</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.302905</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22263442</th>\n",
       "      <td>1</td>\n",
       "      <td>2244</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2_999547</td>\n",
       "      <td>2_999547_2016-02-04 00:00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-02-04</td>\n",
       "      <td>1.373596</td>\n",
       "      <td>-2.910392</td>\n",
       "      <td>...</td>\n",
       "      <td>5</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "      <td>0.468523</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>471360 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          store_nbr  item_nbr  unit_sales  onpromotion    traj_id  \\\n",
       "9280241           0         1         2.0            0  1_1003679   \n",
       "9280242           0         1         1.0            0  1_1003679   \n",
       "9280246           0         1         4.0            0  1_1003679   \n",
       "9280247           0         1         1.0            0  1_1003679   \n",
       "9280248           0         1         4.0            0  1_1003679   \n",
       "...             ...       ...         ...          ...        ...   \n",
       "22263437          1      2244         2.0            0   2_999547   \n",
       "22263438          1      2244         1.0            0   2_999547   \n",
       "22263439          1      2244         7.0            0   2_999547   \n",
       "22263440          1      2244        13.0            0   2_999547   \n",
       "22263442          1      2244        15.0            0   2_999547   \n",
       "\n",
       "                              unique_id  open       date  log_sales       oil  \\\n",
       "9280241   1_1003679_2015-08-21 00:00:00   1.0 2015-08-21  -0.701294 -1.483286   \n",
       "9280242   1_1003679_2015-08-22 00:00:00   1.0 2015-08-22  -1.415077       NaN   \n",
       "9280246   1_1003679_2015-08-26 00:00:00   1.0 2015-08-26   0.012490 -1.798803   \n",
       "9280247   1_1003679_2015-08-27 00:00:00   1.0 2015-08-27  -1.415077 -1.156443   \n",
       "9280248   1_1003679_2015-08-28 00:00:00   1.0 2015-08-28   0.012490 -0.700158   \n",
       "...                                 ...   ...        ...        ...       ...   \n",
       "22263437   2_999547_2016-01-30 00:00:00   1.0 2016-01-30  -0.701294       NaN   \n",
       "22263438   2_999547_2016-01-31 00:00:00   1.0 2016-01-31  -1.415077       NaN   \n",
       "22263439   2_999547_2016-02-01 00:00:00   1.0 2016-02-01   0.588766 -2.912010   \n",
       "22263440   2_999547_2016-02-02 00:00:00   1.0 2016-02-02   1.226235 -3.190312   \n",
       "22263442   2_999547_2016-02-04 00:00:00   1.0 2016-02-04   1.373596 -2.910392   \n",
       "\n",
       "          ...  family  class  perishable  transactions  day_of_week  \\\n",
       "9280241   ...      10     14           0      0.091130            4   \n",
       "9280242   ...      10     14           0     -1.497176            5   \n",
       "9280246   ...      10     14           0     -0.055483            2   \n",
       "9280247   ...      10     14           0     -0.221101            3   \n",
       "9280248   ...      10     14           0      0.194302            4   \n",
       "...       ...     ...    ...         ...           ...          ...   \n",
       "22263437  ...       5    176           0      0.938227            5   \n",
       "22263438  ...       5    176           0      0.813335            6   \n",
       "22263439  ...       5    176           0      0.433227            0   \n",
       "22263440  ...       5    176           0      0.302905            1   \n",
       "22263442  ...       5    176           0      0.468523            3   \n",
       "\n",
       "          day_of_month  month  national_hol  regional_hol  local_hol  \n",
       "9280241             21      8            54             0          2  \n",
       "9280242             22      8            54             0          2  \n",
       "9280246             26      8            54             0          2  \n",
       "9280247             27      8            54             0          2  \n",
       "9280248             28      8            53             0          2  \n",
       "...                ...    ...           ...           ...        ...  \n",
       "22263437            30      1            32             0          2  \n",
       "22263438            31      1            34             0          2  \n",
       "22263439             1      2             6             0          2  \n",
       "22263440             2      2            35             0          2  \n",
       "22263442             4      2             5             0          2  \n",
       "\n",
       "[471360 rows x 24 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867946c-52e4-4cb5-8936-aa31d4c9d64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = None\n",
    "devices = 'cpu'\n",
    "\n",
    "writer = SummaryWriter(log_dir = CONFIG_DICT[\"models\"][\"electricity\"] / \"logs\" )\n",
    "early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=1e-4, patience=10, verbose=False, mode=\"min\")\n",
    "lr_logger = LearningRateMonitor(logging_interval='epoch') \n",
    "logger = TensorBoardLogger(CONFIG_DICT[\"models\"][\"electricity\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63d6f9d-1c0f-41d4-95c5-6ec4699383ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "      timeseries_dict[\"training_dataset\"],\n",
    "      learning_rate=0.001,\n",
    "      hidden_size=160,\n",
    "      attention_head_size=4,\n",
    "      dropout=0.1,\n",
    "      hidden_continuous_size=80,\n",
    "      output_size= 3,  # 7 quantiles by default\n",
    "      loss=QuantileLoss([0.1, 0.5, 0.9]),\n",
    "      log_interval=1,\n",
    "      reduce_on_plateau_patience=1000, # is this after 2 epochs or 2 steps? very important\n",
    "      optimizer=\"adam\"\n",
    "    )\n",
    "\n",
    "# find optimal learning rate\n",
    "res = trainer.tuner.lr_find(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    max_lr=10.0,\n",
    "    min_lr=1e-6,\n",
    ")\n",
    "\n",
    "print(f\"suggested learning rate: {res.suggestion()}\")\n",
    "fig = res.plot(show=True, suggest=True)\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b95ed310-2e86-4d04-8ed2-5c78fd1bc877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-02 17:46:10,739]\u001b[0m A new study created in memory with name: no-name-1f74a184-856c-4b40-be84-c1d6581fb213\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[32m[I 2023-03-02 18:01:48,044]\u001b[0m Trial 0 finished with value: 0.39809274673461914 and parameters: {'gradient_clip_val': 0.03585452814900768, 'hidden_size': 20, 'dropout': 0.21830552478253223, 'hidden_continuous_size': 10, 'attention_head_size': 4, 'learning_rate': 0.0004841484364165627}. Best is trial 0 with value: 0.39809274673461914.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[32m[I 2023-03-02 18:15:02,128]\u001b[0m Trial 1 finished with value: 0.3463554084300995 and parameters: {'gradient_clip_val': 0.0862896309500042, 'hidden_size': 8, 'dropout': 0.25034045345191475, 'hidden_continuous_size': 8, 'attention_head_size': 1, 'learning_rate': 0.006012302863711588}. Best is trial 1 with value: 0.3463554084300995.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 18:17:54,232]\u001b[0m Trial 2 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n",
      "\u001b[32m[I 2023-03-02 18:39:23,165]\u001b[0m Trial 3 finished with value: 0.32733678817749023 and parameters: {'gradient_clip_val': 0.0877641523466769, 'hidden_size': 86, 'dropout': 0.2955762804510075, 'hidden_continuous_size': 12, 'attention_head_size': 1, 'learning_rate': 0.0010078836138748574}. Best is trial 3 with value: 0.32733678817749023.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 18:42:45,258]\u001b[0m Trial 4 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 18:45:45,164]\u001b[0m Trial 5 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 18:50:04,579]\u001b[0m Trial 6 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 18:54:15,255]\u001b[0m Trial 7 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 18:57:27,113]\u001b[0m Trial 8 pruned. Trial was pruned at epoch 1.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "\u001b[32m[I 2023-03-02 19:00:57,067]\u001b[0m Trial 9 pruned. Trial was pruned at epoch 1.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gradient_clip_val': 0.0877641523466769, 'hidden_size': 86, 'dropout': 0.2955762804510075, 'hidden_continuous_size': 12, 'attention_head_size': 1, 'learning_rate': 0.0010078836138748574}\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, DeviceStatsMonitor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import electricity_dataloader\n",
    "from config import *\n",
    "\n",
    "print(\"Preparing dataset...\") \n",
    "  \n",
    "electricity = electricity_dataloader.create_electricity_timeseries_tft()\n",
    "timeseries_dict =  electricity\n",
    "config_name_string = \"electricity\"\n",
    "parameters = []\n",
    "model_dir = CONFIG_DICT[\"models\"][config_name_string]\n",
    "\n",
    "\n",
    "\n",
    "# create study\n",
    "study = optimize_hyperparameters(\n",
    "    electricity[\"train_dataloader\"],\n",
    "    electricity[\"val_dataloader\"],\n",
    "    model_path=\"hypertuning_2\",\n",
    "    #study=\"Desktop/Mein_Ordner/WI-INFO/Semester_4/Information_Systems/TFT_project/RT1_TFT/hypertuning/trial_0/epoch=156.ckpt\"\n",
    "    n_trials=10,\n",
    "    max_epochs=10,\n",
    "    gradient_clip_val_range=(0.01, 0.1),\n",
    "    hidden_size_range=(8, 160),\n",
    "    hidden_continuous_size_range=(8, 80),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.0001, 0.01),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=30, max_epochs=10, log_every_n_steps=30),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=False,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "    optimizer=\"adam\"\n",
    ")\n",
    "\n",
    "# save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)\n",
    "\n",
    "\n",
    "#first run : 0.000741"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
