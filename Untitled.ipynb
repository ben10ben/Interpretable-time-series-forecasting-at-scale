{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b496d73-f197-4f04-b2e0-2bf06518179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, DeviceStatsMonitor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import retail_dataloader, retail_formatter\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ac9adb-4358-4610-ae29-24a67cb5b740",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\dataloading_helpers\\retail_dataloader.py:392: DtypeWarning: Columns (5,6,23,24) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  retail_data = pd.read_csv(csv_file, index_col=0)\n"
     ]
    }
   ],
   "source": [
    "retail = retail_dataloader.create_retail_timeseries_tft()\n",
    "timeseries_dict =  retail\n",
    "config_name_string = \"retail\"\n",
    "parameters = []\n",
    "model_dir = CONFIG_DICT[\"models\"][config_name_string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5c1af6a-a1f7-4651-b212-03d871453b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting train-valid-test splits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\dataloading_helpers\\retail_formatter.py:84: FutureWarning: The pandas.datetime class is deprecated and will be removed from pandas in a future version. Import from datetime module instead.\n",
      "  valid_boundary = pd.datetime(2015, 12, 1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting scalers with training data...\n",
      "Setting scalers with training data...\n"
     ]
    }
   ],
   "source": [
    "standardizer = retail_formatter.FavoritaFormatter()\n",
    "train, test, validation = standardizer.split_data(df=retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5867946c-52e4-4cb5-8936-aa31d4c9d64a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4382f-1fde-48e0-a76d-ac34a4108470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing modules...\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:31:09,151]\u001b[0m A new study created in memory with name: no-name-2de26a1c-68a9-49a7-8d22-474de4bdc72e\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6734ba03be43eb80359956c99b3c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Restoring states from the checkpoint path at C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\.lr_find_8adb9c78-ec14-4253-ad54-6616c2e6a44f.ckpt\n",
      "Restored all states from the checkpoint file at C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\.lr_find_8adb9c78-ec14-4253-ad54-6616c2e6a44f.ckpt\n",
      "\u001b[32m[I 2023-03-04 14:33:21,639]\u001b[0m Using learning rate of 0.00198\u001b[0m\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing modules...\")\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, DeviceStatsMonitor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import electricity_dataloader\n",
    "from config import *\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "print(\"Preparing dataset...\") \n",
    "  \n",
    "electricity = electricity_dataloader.create_electricity_timeseries_tft()\n",
    "timeseries_dict =  electricity\n",
    "config_name_string = \"electricity\"\n",
    "parameters = []\n",
    "model_dir = CONFIG_DICT[\"models\"][config_name_string]\n",
    "\n",
    "\n",
    "\n",
    "  # create study\n",
    "study = optimize_hyperparameters(\n",
    "    electricity[\"train_dataloader\"],\n",
    "    electricity[\"val_dataloader\"],\n",
    "    model_path=\"hyperparameters_electricity\",\n",
    "    n_trials=100,\n",
    "    max_epochs=20,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(16, 256),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.0005, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=40, max_epochs=20, log_every_n_steps=5),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=True,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "    optimizer=\"adam\"\n",
    "  )\n",
    "\n",
    "  # save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
