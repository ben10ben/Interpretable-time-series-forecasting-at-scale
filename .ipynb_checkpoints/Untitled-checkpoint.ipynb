{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8df02c81-5770-40a1-b351-cae02ca1881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataloading_helpers import utils, base\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import sklearn\n",
    "\n",
    "GenericDataFormatter = base.GenericDataFormatter\n",
    "DataTypes = base.DataTypes\n",
    "InputTypes = base.InputTypes\n",
    "\n",
    "\n",
    "class ElectricityFormatter(GenericDataFormatter):\n",
    "    _column_definition = [\n",
    "      ('id', DataTypes.REAL_VALUED, InputTypes.ID),\n",
    "      ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.TIME),\n",
    "      ('power_usage', DataTypes.REAL_VALUED, InputTypes.TARGET),\n",
    "      ('hour', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('day_of_week', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('hours_from_start', DataTypes.REAL_VALUED, InputTypes.KNOWN_INPUT),\n",
    "      ('categorical_id', DataTypes.CATEGORICAL, InputTypes.STATIC_INPUT),\n",
    "  ]\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialises formatter.\"\"\"\n",
    "\n",
    "        self.identifiers = None\n",
    "        self._real_scalers = None\n",
    "        self._cat_scalers = None\n",
    "        self._target_scaler = None\n",
    "        self._num_classes_per_cat_input = None\n",
    "        self._time_steps = self.get_fixed_params()['total_time_steps']\n",
    "\n",
    "    def split_data(self, df, valid_boundary=1315, test_boundary=1339):\n",
    "        index = df['days_from_start']\n",
    "        train = df.loc[index < valid_boundary]\n",
    "        valid = df.loc[(index >= valid_boundary - 7) & (index < test_boundary)]\n",
    "        test = df.loc[index >= test_boundary - 7]\n",
    "\n",
    "        self.set_scalers(train)\n",
    "\n",
    "        return (self.transform_inputs(data) for data in [train, valid, test])\n",
    "\n",
    "    def set_scalers(self, df):\n",
    "        column_definitions = self.get_column_definition()\n",
    "        id_column = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                   column_definitions)\n",
    "        target_column = utils.get_single_col_by_input_type(InputTypes.TARGET,\n",
    "                                                       column_definitions)\n",
    "\n",
    "        # Format real scalers\n",
    "        real_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.REAL_VALUED, column_definitions,\n",
    "            {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "        # Initialise scaler caches\n",
    "        self._real_scalers = {}\n",
    "        self._target_scaler = {}\n",
    "        identifiers = []\n",
    "        for identifier, sliced in df.groupby(id_column):\n",
    "            if len(sliced) >= self._time_steps:\n",
    "\n",
    "                data = sliced[real_inputs].values\n",
    "                targets = sliced[[target_column]].values\n",
    "                self._real_scalers[identifier] \\\n",
    "              = sklearn.preprocessing.StandardScaler().fit(data)\n",
    "\n",
    "                self._target_scaler[identifier] \\\n",
    "              = sklearn.preprocessing.StandardScaler().fit(targets)\n",
    "                identifiers.append(identifier)\n",
    "\n",
    "        # Format categorical scalers\n",
    "        categorical_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.CATEGORICAL, column_definitions,\n",
    "            {InputTypes.ID, InputTypes.TIME})\n",
    "\n",
    "        categorical_scalers = {}\n",
    "        num_classes = []\n",
    "        for col in categorical_inputs:\n",
    "            srs = df[col].apply(str)\n",
    "            categorical_scalers[col] = sklearn.preprocessing.LabelEncoder().fit(\n",
    "                srs.values)\n",
    "            num_classes.append(srs.nunique())\n",
    "\n",
    "        # Set categorical scaler outputs\n",
    "        self._cat_scalers = categorical_scalers\n",
    "        self._num_classes_per_cat_input = num_classes\n",
    "\n",
    "        # Extract identifiers in case required\n",
    "        self.identifiers = identifiers\n",
    "\n",
    "    def transform_inputs(self, df):\n",
    "        if self._real_scalers is None and self._cat_scalers is None:\n",
    "            raise ValueError('Scalers have not been set!')\n",
    "\n",
    "        column_definitions = self.get_column_definition()\n",
    "        id_col = utils.get_single_col_by_input_type(InputTypes.ID,\n",
    "                                                column_definitions)\n",
    "        real_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.REAL_VALUED, column_definitions,\n",
    "            {InputTypes.ID, InputTypes.TIME})\n",
    "        categorical_inputs = utils.extract_cols_from_data_type(\n",
    "            DataTypes.CATEGORICAL, column_definitions,\n",
    "            {InputTypes.ID, InputTypes.TIME})\n",
    "    \n",
    "    # Transform real inputs per entity\n",
    "        df_list = []\n",
    "        for identifier, sliced in df.groupby(id_col):\n",
    "\n",
    "          # Filter out any trajectories that are too short\n",
    "          if len(sliced) >= self._time_steps:\n",
    "            sliced_copy = sliced.copy()\n",
    "            sliced_copy[real_inputs] = self._real_scalers[identifier].transform(\n",
    "                sliced_copy[real_inputs].values)\n",
    "            df_list.append(sliced_copy)\n",
    "\n",
    "        output = pd.concat(df_list, axis=0)\n",
    "\n",
    "    # Format categorical inputs\n",
    "        for col in categorical_inputs:\n",
    "            string_df = df[col].apply(str)\n",
    "            output[col] = self._cat_scalers[col].transform(string_df)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "    def get_fixed_params(self):\n",
    "        \"\"\"Returns fixed model parameters for experiments.\"\"\"\n",
    "\n",
    "        fixed_params = {\n",
    "            'total_time_steps': 8 * 24,\n",
    "            'num_encoder_steps': 7 * 24,\n",
    "            'num_epochs': 100,\n",
    "            'early_stopping_patience': 5,\n",
    "            'multiprocessing_workers': 5\n",
    "        }\n",
    "\n",
    "        return fixed_params\n",
    "    \n",
    "\n",
    "    def format_predictions(self, predictions):\n",
    "        if self._target_scaler is None:\n",
    "            raise ValueError('Scalers have not been set!')\n",
    "\n",
    "        column_names = predictions.columns\n",
    "\n",
    "        df_list = []\n",
    "        for identifier, sliced in predictions.groupby('identifier'):\n",
    "            sliced_copy = sliced.copy()\n",
    "            target_scaler = self._target_scaler[identifier]\n",
    "\n",
    "            for col in column_names:\n",
    "                if col not in {'forecast_time', 'identifier'}:\n",
    "                    sliced_copy[col] = target_scaler.inverse_transform(sliced_copy[col])\n",
    "            df_list.append(sliced_copy)\n",
    "\n",
    "        output = pd.concat(df_list, axis=0)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5cb04b7f-9636-41ec-8aa1-e19a721ea328",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity = pd.read_csv(csv_file, index_col=0)    \n",
    "standardizer = ElectricityFormatter()\n",
    "train, test, validation = standardizer.split_data(df=electricity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43eb1060-9e44-42fe-b3f5-694288ce35c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_usage</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>categorical_id</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>categorical_day_of_week</th>\n",
       "      <th>categorical_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17544</th>\n",
       "      <td>-0.127174</td>\n",
       "      <td>26304.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>-1.661325</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.499719</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.731721</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17545</th>\n",
       "      <td>-0.050713</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>-1.516862</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.499719</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.731062</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17546</th>\n",
       "      <td>-0.050713</td>\n",
       "      <td>26306.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>-1.372399</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.499719</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.730403</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17547</th>\n",
       "      <td>-0.050713</td>\n",
       "      <td>26307.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01 03:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>-1.227936</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.499719</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.729744</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17548</th>\n",
       "      <td>-0.127174</td>\n",
       "      <td>26308.0</td>\n",
       "      <td>1096</td>\n",
       "      <td>0</td>\n",
       "      <td>2014-01-01 04:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>-1.083473</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.499719</td>\n",
       "      <td>1</td>\n",
       "      <td>-1.729085</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460738</th>\n",
       "      <td>0.305581</td>\n",
       "      <td>31555.0</td>\n",
       "      <td>1314</td>\n",
       "      <td>368</td>\n",
       "      <td>2014-08-07 19:00:00</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>1.083473</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>8</td>\n",
       "      <td>1.729085</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460739</th>\n",
       "      <td>0.101327</td>\n",
       "      <td>31556.0</td>\n",
       "      <td>1314</td>\n",
       "      <td>368</td>\n",
       "      <td>2014-08-07 20:00:00</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>1.227936</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>8</td>\n",
       "      <td>1.729744</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460740</th>\n",
       "      <td>-0.303532</td>\n",
       "      <td>31557.0</td>\n",
       "      <td>1314</td>\n",
       "      <td>368</td>\n",
       "      <td>2014-08-07 21:00:00</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>1.372399</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>8</td>\n",
       "      <td>1.730403</td>\n",
       "      <td>3</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460741</th>\n",
       "      <td>0.002848</td>\n",
       "      <td>31558.0</td>\n",
       "      <td>1314</td>\n",
       "      <td>368</td>\n",
       "      <td>2014-08-07 22:00:00</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>1.516862</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>8</td>\n",
       "      <td>1.731062</td>\n",
       "      <td>3</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10460742</th>\n",
       "      <td>-0.730277</td>\n",
       "      <td>31559.0</td>\n",
       "      <td>1314</td>\n",
       "      <td>368</td>\n",
       "      <td>2014-08-07 23:00:00</td>\n",
       "      <td>MT_370</td>\n",
       "      <td>1.661325</td>\n",
       "      <td>7</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>8</td>\n",
       "      <td>1.731721</td>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923536 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          power_usage  time_idx  days_from_start  categorical_id  \\\n",
       "17544       -0.127174   26304.0             1096               0   \n",
       "17545       -0.050713   26305.0             1096               0   \n",
       "17546       -0.050713   26306.0             1096               0   \n",
       "17547       -0.050713   26307.0             1096               0   \n",
       "17548       -0.127174   26308.0             1096               0   \n",
       "...               ...       ...              ...             ...   \n",
       "10460738     0.305581   31555.0             1314             368   \n",
       "10460739     0.101327   31556.0             1314             368   \n",
       "10460740    -0.303532   31557.0             1314             368   \n",
       "10460741     0.002848   31558.0             1314             368   \n",
       "10460742    -0.730277   31559.0             1314             368   \n",
       "\n",
       "                         date      id      hour  day  day_of_week  month  \\\n",
       "17544     2014-01-01 00:00:00  MT_001 -1.661325    1    -0.499719      1   \n",
       "17545     2014-01-01 01:00:00  MT_001 -1.516862    1    -0.499719      1   \n",
       "17546     2014-01-01 02:00:00  MT_001 -1.372399    1    -0.499719      1   \n",
       "17547     2014-01-01 03:00:00  MT_001 -1.227936    1    -0.499719      1   \n",
       "17548     2014-01-01 04:00:00  MT_001 -1.083473    1    -0.499719      1   \n",
       "...                       ...     ...       ...  ...          ...    ...   \n",
       "10460738  2014-08-07 19:00:00  MT_370  1.083473    7     0.002292      8   \n",
       "10460739  2014-08-07 20:00:00  MT_370  1.227936    7     0.002292      8   \n",
       "10460740  2014-08-07 21:00:00  MT_370  1.372399    7     0.002292      8   \n",
       "10460741  2014-08-07 22:00:00  MT_370  1.516862    7     0.002292      8   \n",
       "10460742  2014-08-07 23:00:00  MT_370  1.661325    7     0.002292      8   \n",
       "\n",
       "          hours_from_start  categorical_day_of_week  categorical_hour  \n",
       "17544            -1.731721                        2                 0  \n",
       "17545            -1.731062                        2                 1  \n",
       "17546            -1.730403                        2                 2  \n",
       "17547            -1.729744                        2                 3  \n",
       "17548            -1.729085                        2                 4  \n",
       "...                    ...                      ...               ...  \n",
       "10460738          1.729085                        3                19  \n",
       "10460739          1.729744                        3                20  \n",
       "10460740          1.730403                        3                21  \n",
       "10460741          1.731062                        3                22  \n",
       "10460742          1.731721                        3                23  \n",
       "\n",
       "[1923536 rows x 13 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77968b75-7627-43ab-98e5-3880227abf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from config import *\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer, TorchNormalizer, EncoderNormalizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# set path in config.py\n",
    "txt_file = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014.txt\"\n",
    "csv_file = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014.csv\"\n",
    "csv_file_normalized = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014_normalized.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prep_electricity_data function copied from google paper:\n",
    "https://github.com/google-research/google-research/blob/master/tft/script_download_data.py\n",
    "\n",
    "args:\n",
    "  -txt_file: path to .txt document containg raw electricity dataset\n",
    "      \n",
    "  -output_path: path to save/load prepared csv to/from\n",
    "\n",
    "output: electricity_dataset_dict\n",
    "  -training dataset\n",
    "  -training dataloader\n",
    "  -validation dataloader\n",
    "  -validation dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def prep_electricity_data(txt_file):\n",
    "    df = pd.read_csv(txt_file, index_col=0, sep=';', decimal=',')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Used to determine the start and end dates of a series\n",
    "    output = df.resample('1h').mean().replace(0., np.nan)\n",
    "\n",
    "    earliest_time = output.index.min()\n",
    "\n",
    "\n",
    "    df_list = []\n",
    "    for label in output:\n",
    "        srs = output[label]\n",
    "\n",
    "        start_date = min(srs.fillna(method='ffill').dropna().index)\n",
    "        end_date = max(srs.fillna(method='bfill').dropna().index)\n",
    "\n",
    "        active_range = (srs.index >= start_date) & (srs.index <= end_date)\n",
    "        srs = srs[active_range].fillna(0.)\n",
    "\n",
    "        tmp = pd.DataFrame({'power_usage': srs})\n",
    "        date = tmp.index\n",
    "        tmp['time_idx'] = (date - earliest_time).seconds / 60 / 60 + (\n",
    "          date - earliest_time).days * 24\n",
    "        tmp['days_from_start'] = (date - earliest_time).days\n",
    "        tmp['categorical_id'] = label\n",
    "        tmp['date'] = date\n",
    "        tmp['id'] = label\n",
    "        tmp['hour'] = date.hour\n",
    "        tmp['day'] = date.day\n",
    "        tmp['day_of_week'] = date.dayofweek\n",
    "        tmp['month'] = date.month\n",
    "\n",
    "        df_list.append(tmp)\n",
    "\n",
    "    output = pd.concat(df_list, axis=0, join='outer').reset_index(drop=True)\n",
    "\n",
    "    output['categorical_id'] = output['id'].copy()\n",
    "    output['hours_from_start'] = output['time_idx']\n",
    "    #output['categorical_day_of_week'] = output['day_of_week'].copy()\n",
    "    #output['categorical_hour'] = output['hour'].copy()\n",
    "\n",
    "    # Filter to match range used by other academic papers\n",
    "    output = output[(output['days_from_start'] >= 1096)\n",
    "                    & (output['days_from_start'] < 1346)].copy()\n",
    "\n",
    "    output.to_csv(csv_file)\n",
    "    \n",
    "    \n",
    "    # normalize dataset similar to google implementation\n",
    "    valid_boundary = 1315\n",
    "    #test_boundary = 1339\n",
    "    \n",
    "    real_scaler = {}\n",
    "    cat_scaler = {}\n",
    "    target_scaler = {}\n",
    "    \n",
    "    index = df['days_from_start']\n",
    "    train = df.loc[index < valid_boundary]\n",
    "    \n",
    "    \n",
    "    target = [\"power_usage\"]\n",
    "    real = [\"hour\", \"day_of_week\", \"hours_from_start\"]\n",
    "    categorical = [\"categorical_id\"]\n",
    "    \n",
    "    for i in target:\n",
    "        data = train[i].values\n",
    "        target_scaler[i] = StandardScaler().fit(data)\n",
    "    \n",
    "    for i in real:\n",
    "        data = train[i].values\n",
    "        real_scaler[i] = StandardScaler().fit(data)\n",
    "      \n",
    "    for i in categorical:\n",
    "        data = train[i].values\n",
    "        categorical_scaler[i] = LabelEncoder().fit(data)\n",
    "    \n",
    "    output_normalized.to_csv(csv_file_normalized)\n",
    "    \n",
    "    return output\n",
    "\n",
    "\n",
    "def create_electricity_timeseries_tft():\n",
    "   \n",
    "    try:\n",
    "        electricity_data = pd.read_csv(csv_file, index_col=0)    \n",
    "    except:\n",
    "        electricity_data = prep_electricity_data(txt_file)\n",
    "\n",
    "\n",
    "    electricity_data['time_idx'] = electricity_data['time_idx'].astype('int')\n",
    "    electricity_data['categorical_id'] = electricity_data['categorical_id'].astype('category')\n",
    "  \n",
    "    max_prediction_length = 24\n",
    "    max_encoder_length = 168\n",
    "    training_cutoff = electricity_data[\"time_idx\"].max() - max_prediction_length\n",
    "    \n",
    "\n",
    "    training = TimeSeriesDataSet(\n",
    "      electricity_data[lambda x: x.time_idx <= training_cutoff],\n",
    "      time_idx=\"time_idx\",\n",
    "      target=\"power_usage\",\n",
    "      group_ids=[\"id\"],\n",
    "      min_encoder_length=max_encoder_length,# // 2,  # keep encoder length long (as it is in the validation set)\n",
    "      max_encoder_length=max_encoder_length,\n",
    "      min_prediction_length=max_prediction_length,\n",
    "      max_prediction_length=max_prediction_length,\n",
    "      static_categoricals=[\"categorical_id\"],\n",
    "      static_reals=[],\n",
    "      time_varying_known_categoricals=[],\n",
    "      #variable_groups={\"special_days\": special_days},  # group of categorical variables can be treated as one variable\n",
    "      time_varying_known_reals=[\"time_idx\", \"hour\", \"day_of_week\"],\n",
    "      time_varying_unknown_categoricals=[],\n",
    "      time_varying_unknown_reals=[],\n",
    "      target_normalizer=TorchNormalizer(method=\"standard\", center=\"False\"),\n",
    "      #categorical_encoders={\"categorical_id\": LabelEncoder()},\n",
    "      add_relative_time_idx=False,\n",
    "      add_target_scales=False,\n",
    "      add_encoder_length=False, #\n",
    "    )\n",
    "    \n",
    "\n",
    "  # create validation set (predict=True) which means to predict the last max_prediction_length points in time\n",
    "  # for each series\n",
    "    validation = TimeSeriesDataSet.from_dataset(training, electricity_data, predict=True, stop_randomization=True)\n",
    "\n",
    "  # create dataloaders for model\n",
    "    batch_size = 128  # set this between 32 to 128\n",
    "    train_dataloader = training.to_dataloader(train=True, batch_size=batch_size, num_workers=45, pin_memory=True)\n",
    "    val_dataloader = validation.to_dataloader(train=False, batch_size=batch_size * 20, num_workers=30, pin_memory=True)\n",
    "\n",
    "\n",
    "# output data as dict for easier modularity\n",
    "    return {\"training_dataset\": training, \n",
    "          \"train_dataloader\": train_dataloader,\n",
    "          \"val_dataloader\": val_dataloader, \n",
    "          \"validaton_dataset\": validation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8a67f1a-f70b-47b5-99bd-3338fa854e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:554: UserWarning: This DataLoader will create 45 worker processes in total. Our suggested max number of worker in current system is 16 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:554: UserWarning: This DataLoader will create 30 worker processes in total. Our suggested max number of worker in current system is 16 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'training_dataset': TimeSeriesDataSet[length=2118737](\n",
       " \ttime_idx='time_idx',\n",
       " \ttarget='power_usage',\n",
       " \tgroup_ids=['id'],\n",
       " \tweight=None,\n",
       " \tmax_encoder_length=168,\n",
       " \tmin_encoder_length=168,\n",
       " \tmin_prediction_idx=26304,\n",
       " \tmin_prediction_length=24,\n",
       " \tmax_prediction_length=24,\n",
       " \tstatic_categoricals=['categorical_id'],\n",
       " \tstatic_reals=[],\n",
       " \ttime_varying_known_categoricals=[],\n",
       " \ttime_varying_known_reals=['time_idx', 'hour', 'day_of_week'],\n",
       " \ttime_varying_unknown_categoricals=[],\n",
       " \ttime_varying_unknown_reals=[],\n",
       " \tvariable_groups={},\n",
       " \tconstant_fill_strategy={},\n",
       " \tallow_missing_timesteps=False,\n",
       " \tlags={},\n",
       " \tadd_relative_time_idx=False,\n",
       " \tadd_target_scales=False,\n",
       " \tadd_encoder_length=False,\n",
       " \ttarget_normalizer=TorchNormalizer(method='standard', center='False', transformation=None, method_kwargs={}),\n",
       " \tcategorical_encoders={'__group_id__id': NaNLabelEncoder(add_nan=False, warn=True), 'categorical_id': NaNLabelEncoder(add_nan=False, warn=True)},\n",
       " \tscalers={'time_idx': StandardScaler(), 'hour': StandardScaler(), 'day_of_week': StandardScaler()},\n",
       " \trandomize_length=None,\n",
       " \tpredict_mode=False\n",
       " ),\n",
       " 'train_dataloader': <torch.utils.data.dataloader.DataLoader at 0x2303ca38580>,\n",
       " 'val_dataloader': <torch.utils.data.dataloader.DataLoader at 0x230086892e0>,\n",
       " 'validaton_dataset': TimeSeriesDataSet[length=369](\n",
       " \ttime_idx='time_idx',\n",
       " \ttarget='power_usage',\n",
       " \tgroup_ids=['id'],\n",
       " \tweight=None,\n",
       " \tmax_encoder_length=168,\n",
       " \tmin_encoder_length=168,\n",
       " \tmin_prediction_idx=26304,\n",
       " \tmin_prediction_length=24,\n",
       " \tmax_prediction_length=24,\n",
       " \tstatic_categoricals=['categorical_id'],\n",
       " \tstatic_reals=[],\n",
       " \ttime_varying_known_categoricals=[],\n",
       " \ttime_varying_known_reals=['time_idx', 'hour', 'day_of_week'],\n",
       " \ttime_varying_unknown_categoricals=[],\n",
       " \ttime_varying_unknown_reals=[],\n",
       " \tvariable_groups={},\n",
       " \tconstant_fill_strategy={},\n",
       " \tallow_missing_timesteps=False,\n",
       " \tlags={},\n",
       " \tadd_relative_time_idx=False,\n",
       " \tadd_target_scales=False,\n",
       " \tadd_encoder_length=False,\n",
       " \ttarget_normalizer=TorchNormalizer(method='standard', center='False', transformation=None, method_kwargs={}),\n",
       " \tcategorical_encoders={'__group_id__id': NaNLabelEncoder(add_nan=False, warn=True), 'categorical_id': NaNLabelEncoder(add_nan=False, warn=True)},\n",
       " \tscalers={'time_idx': StandardScaler(), 'hour': StandardScaler(), 'day_of_week': StandardScaler()},\n",
       " \trandomize_length=None,\n",
       " \tpredict_mode=True\n",
       " )}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "electricity = create_electricity_timeseries_tft()\n",
    "\n",
    "timeseries_dict =  electricity\n",
    "config_name_string = \"electricity\"\n",
    "parameters = []\n",
    "electricity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72411a2-02fc-4c6d-9b67-b29c695d75ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec1c8b78-abdd-4f5e-b2cd-5fece94908a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf6183c-ec53-48a7-b640-19bdb5ffcb26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae6b0a4-c8cd-4e32-95e3-1b4c80434831",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from config import *\n",
    "from pytorch_forecasting import TimeSeriesDataSet\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer, TorchNormalizer, EncoderNormalizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "# set path in config.py\n",
    "txt_file = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014.txt\"\n",
    "csv_file = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014.csv\"\n",
    "csv_file_normalized = CONFIG_DICT[\"datasets\"][\"electricity\"] / \"LD2011_2014_normalized.csv\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "prep_electricity_data function copied from google paper:\n",
    "https://github.com/google-research/google-research/blob/master/tft/script_download_data.py\n",
    "\n",
    "args:\n",
    "  -txt_file: path to .txt document containg raw electricity dataset\n",
    "      \n",
    "  -output_path: path to save/load prepared csv to/from\n",
    "\n",
    "output: electricity_dataset_dict\n",
    "  -training dataset\n",
    "  -training dataloader\n",
    "  -validation dataloader\n",
    "  -validation dataset\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def prep_electricity_data(txt_file):\n",
    "    df = pd.read_csv(txt_file, index_col=0, sep=';', decimal=',')\n",
    "    df.index = pd.to_datetime(df.index)\n",
    "    df.sort_index(inplace=True)\n",
    "\n",
    "    # Used to determine the start and end dates of a series\n",
    "    output = df.resample('1h').mean().replace(0., np.nan)\n",
    "\n",
    "    earliest_time = output.index.min()\n",
    "\n",
    "\n",
    "    df_list = []\n",
    "    for label in output:\n",
    "        srs = output[label]\n",
    "\n",
    "        start_date = min(srs.fillna(method='ffill').dropna().index)\n",
    "        end_date = max(srs.fillna(method='bfill').dropna().index)\n",
    "\n",
    "        active_range = (srs.index >= start_date) & (srs.index <= end_date)\n",
    "        srs = srs[active_range].fillna(0.)\n",
    "\n",
    "        tmp = pd.DataFrame({'power_usage': srs})\n",
    "        date = tmp.index\n",
    "        tmp['time_idx'] = (date - earliest_time).seconds / 60 / 60 + (\n",
    "          date - earliest_time).days * 24\n",
    "        tmp['days_from_start'] = (date - earliest_time).days\n",
    "        tmp['categorical_id'] = label\n",
    "        tmp['date'] = date\n",
    "        tmp['id'] = label\n",
    "        tmp['hour'] = date.hour\n",
    "        tmp['day'] = date.day\n",
    "        tmp['day_of_week'] = date.dayofweek\n",
    "        tmp['month'] = date.month\n",
    "\n",
    "        df_list.append(tmp)\n",
    "\n",
    "    output = pd.concat(df_list, axis=0, join='outer').reset_index(drop=True)\n",
    "\n",
    "    output['categorical_id'] = output['id'].copy()\n",
    "    output['hours_from_start'] = output['time_idx']\n",
    "    #output['categorical_day_of_week'] = output['day_of_week'].copy()\n",
    "    #output['categorical_hour'] = output['hour'].copy()\n",
    "\n",
    "    # Filter to match range used by other academic papers\n",
    "    output = output[(output['days_from_start'] >= 1096)\n",
    "                    & (output['days_from_start'] < 1346)].copy()\n",
    "\n",
    "    return output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "018cbcef-5fa5-4f80-8bcd-24fd009b2d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "electricity_data = pd.read_csv(csv_file, index_col=0)    \n",
    "electricity_data['time_idx'] = electricity_data['time_idx'].astype('int')\n",
    "electricity_data['categorical_id'] = electricity_data['categorical_id'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9ec812e-bbc3-4ef9-ae2e-23b6c26b72c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  0,   0,   0, ..., 368, 368, 368], dtype=int64)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize dataset similar to google implementation\n",
    "valid_boundary = 1315\n",
    "    \n",
    "real_scaler = {}\n",
    "cat_scaler = {}\n",
    "target_scaler = {}\n",
    "    \n",
    "index = electricity_data['days_from_start']\n",
    "train = electricity_data.loc[index < valid_boundary]\n",
    "    \n",
    "    \n",
    "real = [\"hour\", \"day_of_week\", \"hours_from_start\", \"power_usage\"]\n",
    "categorical = [\"categorical_id\"]\n",
    "\n",
    "for each customer in train.groupby(\"categorical_id\")\n",
    "    scaler = StandardScaler()\n",
    "    X_numeric_std = pd.DataFrame(data=scaler.fit_transform(train[real]), columns=real)\n",
    "\n",
    "\n",
    "cat_encoder = LabelEncoder()\n",
    "X_std = pd.merge(X_numeric_std, X_cat_std[categorical], left_index=True, right_index=True)\n",
    "\n",
    "LabelEncoder().fit_transform(X_std[categorical])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "773785e6-bac7-4b61-b7ec-e713b1a25611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hour</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>power_usage</th>\n",
       "      <th>categorical_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.661328</td>\n",
       "      <td>-0.499545</td>\n",
       "      <td>-1.745523</td>\n",
       "      <td>-0.178208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.516865</td>\n",
       "      <td>-0.499545</td>\n",
       "      <td>-1.744862</td>\n",
       "      <td>-0.178113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.372402</td>\n",
       "      <td>-0.499545</td>\n",
       "      <td>-1.744201</td>\n",
       "      <td>-0.178113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.227939</td>\n",
       "      <td>-0.499545</td>\n",
       "      <td>-1.743540</td>\n",
       "      <td>-0.178113</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1.083476</td>\n",
       "      <td>-0.499545</td>\n",
       "      <td>-1.742879</td>\n",
       "      <td>-0.178208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923531</th>\n",
       "      <td>1.083467</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>1.725580</td>\n",
       "      <td>5.370831</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923532</th>\n",
       "      <td>1.227930</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>1.726241</td>\n",
       "      <td>5.143978</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923533</th>\n",
       "      <td>1.372393</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>1.726902</td>\n",
       "      <td>4.694323</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923534</th>\n",
       "      <td>1.516856</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>1.727563</td>\n",
       "      <td>5.034602</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1923535</th>\n",
       "      <td>1.661318</td>\n",
       "      <td>0.002468</td>\n",
       "      <td>1.728224</td>\n",
       "      <td>4.220362</td>\n",
       "      <td>368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1923536 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             hour  day_of_week  hours_from_start  power_usage  categorical_id\n",
       "0       -1.661328    -0.499545         -1.745523    -0.178208               0\n",
       "1       -1.516865    -0.499545         -1.744862    -0.178113               0\n",
       "2       -1.372402    -0.499545         -1.744201    -0.178113               0\n",
       "3       -1.227939    -0.499545         -1.743540    -0.178113               0\n",
       "4       -1.083476    -0.499545         -1.742879    -0.178208               0\n",
       "...           ...          ...               ...          ...             ...\n",
       "1923531  1.083467     0.002468          1.725580     5.370831             368\n",
       "1923532  1.227930     0.002468          1.726241     5.143978             368\n",
       "1923533  1.372393     0.002468          1.726902     4.694323             368\n",
       "1923534  1.516856     0.002468          1.727563     5.034602             368\n",
       "1923535  1.661318     0.002468          1.728224     4.220362             368\n",
       "\n",
       "[1923536 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
