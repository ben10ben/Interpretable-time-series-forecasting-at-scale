{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd8529a1-626f-43f1-a210-3470e81273d8",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b496d73-f197-4f04-b2e0-2bf06518179c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, DeviceStatsMonitor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import electricity_dataloader\n",
    "from config import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1849c23-368c-4da9-ba15-1e4d26dbb0ef",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "In the following we will take a short look into the electricity dataset.\n",
    "To not create bias, we will just look at the train dataset.\n",
    "Because the dataset is not too complex, we will mostly look at the distribution of the target value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62ac9adb-4358-4610-ae29-24a67cb5b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _, _ = electricity_dataloader.create_electricity_dataframe_not_normalized()  \n",
    "print(train.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19332a46-5820-44c5-8082-c2b1d67eebc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>power_usage</th>\n",
       "      <th>time_idx</th>\n",
       "      <th>days_from_start</th>\n",
       "      <th>categorical_id</th>\n",
       "      <th>date</th>\n",
       "      <th>id</th>\n",
       "      <th>hour</th>\n",
       "      <th>day</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>month</th>\n",
       "      <th>hours_from_start</th>\n",
       "      <th>categorical_day_of_week</th>\n",
       "      <th>categorical_hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>17544</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>26304</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 00:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26304.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17545</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>26305</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 01:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26305.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17546</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>26306</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 02:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26306.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17547</th>\n",
       "      <td>2.855330</td>\n",
       "      <td>26307</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 03:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26307.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17548</th>\n",
       "      <td>2.538071</td>\n",
       "      <td>26308</td>\n",
       "      <td>1096</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>2014-01-01 04:00:00</td>\n",
       "      <td>MT_001</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>26308.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       power_usage  time_idx  days_from_start categorical_id  \\\n",
       "17544     2.538071     26304             1096         MT_001   \n",
       "17545     2.855330     26305             1096         MT_001   \n",
       "17546     2.855330     26306             1096         MT_001   \n",
       "17547     2.855330     26307             1096         MT_001   \n",
       "17548     2.538071     26308             1096         MT_001   \n",
       "\n",
       "                      date      id  hour  day  day_of_week  month  \\\n",
       "17544  2014-01-01 00:00:00  MT_001     0    1            2      1   \n",
       "17545  2014-01-01 01:00:00  MT_001     1    1            2      1   \n",
       "17546  2014-01-01 02:00:00  MT_001     2    1            2      1   \n",
       "17547  2014-01-01 03:00:00  MT_001     3    1            2      1   \n",
       "17548  2014-01-01 04:00:00  MT_001     4    1            2      1   \n",
       "\n",
       "       hours_from_start  categorical_day_of_week  categorical_hour  \n",
       "17544           26304.0                        2                 0  \n",
       "17545           26305.0                        2                 1  \n",
       "17546           26306.0                        2                 2  \n",
       "17547           26307.0                        2                 3  \n",
       "17548           26308.0                        2                 4  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77047f60-5bcd-408d-b443-0819f992470d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.53807107e+00, 2.85532995e+00, 3.17258883e+00, ...,\n",
       "       1.78918919e+04, 2.08783784e+04, 1.94459459e+04])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.power_usage.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b2d5ad-81b3-41af-bea2-e4a00e137afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.displot(train, x=\"flipper_length_mm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a4382f-1fde-48e0-a76d-ac34a4108470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing modules...\n",
      "Preparing dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-04 14:31:09,151]\u001b[0m A new study created in memory with name: no-name-2de26a1c-68a9-49a7-8d22-474de4bdc72e\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6734ba03be43eb80359956c99b3c83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Restoring states from the checkpoint path at C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\.lr_find_8adb9c78-ec14-4253-ad54-6616c2e6a44f.ckpt\n",
      "Restored all states from the checkpoint file at C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\.lr_find_8adb9c78-ec14-4253-ad54-6616c2e6a44f.ckpt\n",
      "\u001b[32m[I 2023-03-04 14:33:21,639]\u001b[0m Using learning rate of 0.00198\u001b[0m\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n",
      "\u001b[32m[I 2023-03-04 15:05:54,553]\u001b[0m Trial 0 finished with value: 0.28909581899642944 and parameters: {'gradient_clip_val': 0.3212525138750618, 'hidden_size': 16, 'dropout': 0.10785436359834565, 'hidden_continuous_size': 15, 'attention_head_size': 2, 'learning_rate': 0.0019826123320599316}. Best is trial 0 with value: 0.28909581899642944.\u001b[0m\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'loss' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['loss'])`.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\utilities\\parsing.py:262: UserWarning: Attribute 'logging_metrics' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['logging_metrics'])`.\n",
      "  rank_zero_warn(\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\pytorch_lightning\\loops\\utilities.py:94: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c3d81cdd89c47ed9b2ae380ec4ee4b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Restoring states from the checkpoint path at C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\.lr_find_4a6b8276-4874-47b0-a010-a13b22d7c48e.ckpt\n",
      "Restored all states from the checkpoint file at C:\\Users\\Benedikt\\Desktop\\Mein_Ordner\\WI-INFO\\Semester_4\\Information_Systems\\TFT_project\\RT1_TFT\\.lr_find_4a6b8276-4874-47b0-a010-a13b22d7c48e.ckpt\n",
      "\u001b[32m[I 2023-03-04 15:16:48,595]\u001b[0m Using learning rate of 0.000896\u001b[0m\n",
      "C:\\Users\\Benedikt\\anaconda_main\\lib\\site-packages\\optuna\\integration\\pytorch_lightning.py:52: UserWarning: The metric 'val_loss' is not in the evaluation logs for pruning. Please make sure you set the correct metric name.\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "print(\"Importing modules...\")\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import tensorboard as tb\n",
    "import json\n",
    "import time\n",
    "import warnings\n",
    "from torch import nn\n",
    "from pytorch_lightning.accelerators import *\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from lightning.pytorch.accelerators import find_usable_cuda_devices\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting import Baseline, TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import SMAPE, PoissonLoss, QuantileLoss\n",
    "from pytorch_forecasting.data.encoders import GroupNormalizer\n",
    "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, DeviceStatsMonitor\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from dataloading_helpers import electricity_dataloader\n",
    "from config import *\n",
    "import pickle\n",
    "from pytorch_forecasting.models.temporal_fusion_transformer.tuning import optimize_hyperparameters\n",
    "\n",
    "print(\"Preparing dataset...\") \n",
    "  \n",
    "electricity = electricity_dataloader.create_electricity_timeseries_tft()\n",
    "timeseries_dict =  electricity\n",
    "config_name_string = \"electricity\"\n",
    "parameters = []\n",
    "model_dir = CONFIG_DICT[\"models\"][config_name_string]\n",
    "\n",
    "\n",
    "\n",
    "  # create study\n",
    "study = optimize_hyperparameters(\n",
    "    electricity[\"train_dataloader\"],\n",
    "    electricity[\"val_dataloader\"],\n",
    "    model_path=\"hyperparameters_electricity\",\n",
    "    n_trials=100,\n",
    "    max_epochs=20,\n",
    "    gradient_clip_val_range=(0.01, 1.0),\n",
    "    hidden_size_range=(16, 256),\n",
    "    hidden_continuous_size_range=(8, 128),\n",
    "    attention_head_size_range=(1, 4),\n",
    "    learning_rate_range=(0.0005, 0.1),\n",
    "    dropout_range=(0.1, 0.3),\n",
    "    trainer_kwargs=dict(limit_train_batches=40, max_epochs=20, log_every_n_steps=5),\n",
    "    reduce_on_plateau_patience=4,\n",
    "    use_learning_rate_finder=True,  # use Optuna to find ideal learning rate or use in-built learning rate finder\n",
    "    optimizer=\"adam\"\n",
    "  )\n",
    "\n",
    "  # save study results - also we can resume tuning at a later point in time\n",
    "with open(\"test_study.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(study, fout)\n",
    "\n",
    "# show best hyperparameters\n",
    "print(study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
